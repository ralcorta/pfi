# 4_evaluar_modelo.py  â† PASO 4 (versiÃ³n robusta con resoluciÃ³n de rutas + figuras)
import argparse
import json
from datetime import datetime
from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, average_precision_score
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utilidades
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _savefig(path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    print(f"   â†³ Figura guardada: {path.name}")

def _print_headline(text):
    print("\n" + "="*70)
    print(text)
    print("="*70)

def resolve_here_first(p: Path, script_dir: Path) -> Path:
    """
    Si p es absoluto y existe â†’ p.
    Si p relativo existe respecto del CWD â†’ p.
    Si no, prueba en la carpeta del script: script_dir / p.name.
    Devuelve el primero que exista; si ninguno existe, devuelve p tal cual.
    """
    if p.is_absolute() and p.exists():
        return p
    if p.exists():
        return p
    alt = script_dir / p.name
    if alt.exists():
        return alt
    return p  # se validarÃ¡ mÃ¡s abajo

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(description="EvaluaciÃ³n de modelo detector de ransomware")
parser.add_argument("--model",  default="convlstm_model.keras", help="Ruta al modelo .keras")
parser.add_argument("--x",      default="X_test.npy",           help="Ruta a X de evaluaciÃ³n")
parser.add_argument("--y",      default="y_test.npy",           help="Ruta a y (one-hot)")
parser.add_argument("--tag",    default="base",                 help="Etiqueta para sufijos (base/attack/advtrain)")
parser.add_argument("--outdir", default=".",                    help="Directorio de salida para JSON/figuras/NPY")
args = parser.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) Resolver rutas de forma robusta
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRIPT_DIR = Path(__file__).resolve().parent
out_dir = Path(args.outdir)
tag = args.tag

# Candidatos de modelo: primero el explÃ­cito, luego best, luego final en la carpeta del script
candidate_models = [
    Path(args.model),
    SCRIPT_DIR / "convlstm_model_best.keras",
    SCRIPT_DIR / "convlstm_model.keras",
]

model_path = None
for cand in candidate_models:
    cand_res = resolve_here_first(cand, SCRIPT_DIR)
    if cand_res.exists():
        model_path = cand_res
        break

if model_path is None:
    tried = ", ".join(str(resolve_here_first(c, SCRIPT_DIR)) for c in candidate_models)
    raise FileNotFoundError(f"No encontrÃ© un modelo .keras. ProbÃ©: {tried}")

x_path = resolve_here_first(Path(args.x), SCRIPT_DIR)
y_path = resolve_here_first(Path(args.y), SCRIPT_DIR)

if not x_path.exists() or not y_path.exists():
    raise FileNotFoundError(f"Faltan archivos de eval: {x_path} / {y_path}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) Cargar modelo y datos
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_print_headline("ğŸ” Cargando modelo y datos de evaluaciÃ³n")
print(f"  - Modelo: {model_path}")
print(f"  - X_eval: {x_path}")
print(f"  - y_eval: {y_path}")

model = load_model(model_path)
X_eval = np.load(x_path)
y_eval = np.load(y_path)

print(f"  - X_eval shape: {X_eval.shape}")
print(f"  - y_eval shape: {y_eval.shape}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) EvaluaciÃ³n bÃ¡sica
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_print_headline("ğŸ¯ Evaluando modelo")
loss, accuracy = model.evaluate(X_eval, y_eval, verbose=0)
print(f"  - Loss: {loss:.4f}")
print(f"  - Accuracy: {accuracy:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) Predicciones y mÃ©tricas
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”® Generando predicciones...")
y_pred_proba = model.predict(X_eval, verbose=0)          # (N, 2)
y_pred = np.argmax(y_pred_proba, axis=1)                 # (N,)
y_true = np.argmax(y_eval, axis=1)                       # (N,)

print("\nğŸ“Š Reporte de clasificaciÃ³n")
class_names = ["Benigno", "Ransomware/Malware"]
report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
print(classification_report(y_true, y_pred, target_names=class_names))

# MÃ©tricas especÃ­ficas para clase malware (1)
ransomware_metrics = {
    "precision": report["Ransomware/Malware"]["precision"],
    "recall":    report["Ransomware/Malware"]["recall"],
    "f1_score":  report["Ransomware/Malware"]["f1-score"],
    "support":   int(report["Ransomware/Malware"]["support"])
}

# Matriz de confusiÃ³n + derivadas
cm = confusion_matrix(y_true, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp) if (tn + fp) else 0.0
sensitivity = tp / (tp + fn) if (tp + fn) else 0.0

print("\nğŸ“ˆ MÃ©tricas adicionales")
print(f"  - TP={tp}  TN={tn}  FP={fp}  FN={fn}")
print(f"  - Sensitivity/Recall: {sensitivity:.4f}")
print(f"  - Specificity:        {specificity:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) Curvas ROC y Precisionâ€“Recall
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“ˆ Curvas ROC y Precisionâ€“Recall")
fpr, tpr, _ = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)

prec_curve, rec_curve, _ = precision_recall_curve(y_true, y_pred_proba[:, 1])
avg_precision = average_precision_score(y_true, y_pred_proba[:, 1])

print(f"  - ROC AUC:          {roc_auc:.4f}")
print(f"  - Average Precision {avg_precision:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) Figuras: Confusion matrix, ROC, PR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ–¼ï¸ Generando figuras...")
out_dir.mkdir(parents=True, exist_ok=True)

# Matriz de confusiÃ³n
plt.figure()
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicho'); plt.ylabel('Real'); plt.title(f'Matriz de confusiÃ³n ({tag})')
_savefig(out_dir / f'fig_confusion_matrix_{tag}.png')

# ROC
plt.figure()
plt.plot(fpr, tpr, label=f'AUC={roc_auc:.3f}')
plt.plot([0,1], [0,1], '--')
plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f'ROC ({tag})'); plt.legend()
_savefig(out_dir / f'fig_roc_{tag}.png')

# Precisionâ€“Recall
plt.figure()
plt.plot(rec_curve, prec_curve, label=f'AP={avg_precision:.3f}')
plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title(f'Precisionâ€“Recall ({tag})'); plt.legend()
_savefig(out_dir / f'fig_pr_{tag}.png')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) (Opcional) nombres de features
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
feature_names = []
feat_names_path = resolve_here_first(Path('ransomware_feature_names.txt'), SCRIPT_DIR)
if feat_names_path.exists():
    try:
        with open(feat_names_path, 'r', encoding='utf-8') as f:
            feature_names = [line.strip() for line in f if line.strip()]
        print(f"\nğŸ” Nombres de features ({len(feature_names)}):")
        for i, name in enumerate(feature_names[:10]):
            print(f"  {i+1:2d}. {name}")
        if len(feature_names) > 10:
            print(f"  ... (+{len(feature_names)-10} mÃ¡s)")
    except Exception as e:
        print(f"[WARN] No pude leer {feat_names_path.name}: {e}")
else:
    print("\n[INFO] No se encontrÃ³ ransomware_feature_names.txt (ok si no usaste esas features).")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) Guardar resultados y predicciones
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ’¾ Guardando artefactos de evaluaciÃ³n...")

results = {
    "timestamp": datetime.now().isoformat(),
    "tag": tag,
    "model_name": model_path.name,
    "basic_metrics": {
        "loss": float(loss),
        "accuracy": float(accuracy)
    },
    "ransomware_specific_metrics": ransomware_metrics,
    "confusion_matrix": {
        "true_positives": int(tp),
        "true_negatives": int(tn),
        "false_positives": int(fp),
        "false_negatives": int(fn),
        "sensitivity": float(sensitivity),
        "specificity": float(specificity)
    },
    "roc_auc": float(roc_auc),
    "average_precision": float(avg_precision),
    "classification_report": report,
    "feature_names": feature_names
}

out_json = out_dir / f"evaluation_results_{tag}.json"
with open(out_json, "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

np.save(out_dir / f"y_pred_proba_{tag}.npy", y_pred_proba)
np.save(out_dir / f"y_pred_{tag}.npy", y_pred)
np.save(out_dir / f"y_true_{tag}.npy", y_true)

print("âœ… Guardado:")
print(f"  - {out_json.name}")
print(f"  - y_pred_proba_{tag}.npy")
print(f"  - y_pred_{tag}.npy")
print(f"  - y_true_{tag}.npy")
print(f"  - fig_confusion_matrix_{tag}.png")
print(f"  - fig_roc_{tag}.png")
print(f"  - fig_pr_{tag}.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9) Resumen final
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_print_headline("ğŸ¯ RESUMEN DE EVALUACIÃ“N â€“ DETECTOR DE RANSOMWARE")
print(f"Accuracy (global):           {accuracy:.4f}")
print(f"Precision (Malware):         {ransomware_metrics['precision']:.4f}")
print(f"Recall/Sensitivity (Malware):{ransomware_metrics['recall']:.4f}")
print(f"F1-Score (Malware):          {ransomware_metrics['f1_score']:.4f}")
print(f"ROC AUC:                     {roc_auc:.4f}")
print(f"Average Precision (PR AUC):  {avg_precision:.4f}")
print(f"TP/TN/FP/FN:                 {tp}/{tn}/{fp}/{fn}")
print(f"Specificity:                 {specificity:.4f}")
print("="*70)

# SemÃ¡foro rÃ¡pido por F1 (Malware)
f1 = ransomware_metrics['f1_score']
if f1 > 0.8:
    print("âœ… EXCELENTE: Alta capacidad de detecciÃ³n de ransomware")
elif f1 > 0.7:
    print("âœ… BUENO: Buena capacidad de detecciÃ³n de ransomware")
elif f1 > 0.6:
    print("âš ï¸ REGULAR: Capacidad moderada de detecciÃ³n")
else:
    print("âŒ MEJORABLE: Capacidad baja; revisar datos/arquitectura/umbral")